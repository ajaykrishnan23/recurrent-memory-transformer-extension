# Extending Performance of Recurrent Memory Transformer with LoRA

- Extended GPT2's context length based off the paper [Recurrent Memory Transformer](https://arxiv.org/pdf/2207.06881)
- Applied LoRA on memory tokens to improve perplexity,
